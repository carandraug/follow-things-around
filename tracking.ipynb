{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# VGG Chimpanzee tracking\n",
        "\n",
        "This project provides an interface to track chimpanzees in the wild.\n",
        "It will detect the chimpanzees in video files.  It is based in the\n",
        "following projects:\n",
        "\n",
        "- [Count, Crop and Recognise: Fine-Grained Recognition in the\n",
        "  Wild](https://www.robots.ox.ac.uk/~vgg/research/ccr/) -\n",
        "  [[paper]](http://www.robots.ox.ac.uk/~vgg/publications/2019/Bain19/bain19.pdf)\n",
        "\n",
        "- [Chimpanzee face recognition from videos in the wild using deep\n",
        "  learning](https://www.robots.ox.ac.uk/~vgg/research/ChimpanzeeFaces/) -\n",
        "  [[paper]](https://advances.sciencemag.org/content/advances/5/9/eaaw0736.full.pdf)\n"
      ],
      "metadata": {
        "id": "AR4oetmCFHUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - Read Me First\n"
      ],
      "metadata": {
        "id": "O3vImnLxFHUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project is a [Jupyter](https://jupyter.org/) notebook to track\n",
        "chimpanzees in the wild and was designed to run in [Google\n",
        "Colab](https://colab.research.google.com/).  If you are not reading\n",
        "this notebook in Google Colab, click\n",
        "[here](https://colab.research.google.com/github/ox-vgg/chimpanzee-tracking/blob/main/tracking.ipynb).\n",
        "\n",
        "\n",
        "### 1.1 - What is, and how to use, a Jupyter notebook\n",
        "\n",
        "A Jupyter notebook is a series of \"cells\".  Each cell contains either\n",
        "text (like this one) or code (like others below).  A cell that\n",
        "contains code will have a \"Run cell\" button on the left side like this\n",
        "\"<img height=\"18rem\" alt=\"The 'Run cell' button in Colab\"\n",
        "src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAQAAAD9CzEMAAABTklEQVRYw+2XMU7DMBRAX6ss3VA7VV25AFNWzsDQXoAzVDlBKw6QDJwhTO3OCVjaka0VXVKJDUVC4jOgiMHYcRx9S0j9f7XfS5x8+xsu8R9iQEpGyY4TgnBiR0lGyqA/fMaaI2LJI2tm4fAxObUV3mRNzjgEP+fcCm/yzLwbPKHwhjdZkPjiR2w64wVhw8jv6bdBeEHY+rxFEYz/WaiWWPTCC8LChZ9Q9RZUTOyCvDdeEHJ71drL6o43b0Ftq+6VYxJc8ciXp2L1F37IwSkAuOXVS3BgaApS55TfInzg00ORmoLMSwBww0urIDMFpbcAEpZ8OMeXpmDfQQBwzbNj/N6cUHUUANzzbi03I+oAAUx5stRCfIH6Eql/ZPXfVL3Q1LcK9c1OfbuOcOCoH5kRDn31tiVC4xWhdVRvfiO07xEuIFGuUBEugVGusZfQj28NImRviDLNnQAAAABJRU5ErkJggg==\">\".\n",
        "When you click the \"Run cell\" button, the code in that cell will run\n",
        "and when it finishes, a green check mark appears next to the \"Run\n",
        "cell\" button\".  You need to wait for the code in that cell to finish\n",
        "before \"running\" the next cell.\n",
        "\n",
        "\n",
        "### 1.2 - Particulars of this notebook\n",
        "\n",
        "This notebook was designed to run in Google Colab and to analyse\n",
        "videos in Google Drive.  It will also save back the analysis results\n",
        "in Google Drive.  As such, it requires a Google account.\n",
        "\n",
        "\n",
        "### 1.3 - Results files\n",
        "\n",
        "This notebook will save all the results in a single directory.  It\n",
        "will generate the following files:\n",
        "\n",
        "- `frames` - a directory with the individual video frames.  You may\n",
        "  want to delete them after validating the results.  They take up a\n",
        "  lot of space and can be regenerated later\n",
        "\n",
        "- `detections.pkl` - the initial detections in [Python's pickle\n",
        "  format](https://docs.python.org/3/library/pickle.html).\n",
        "\n",
        "- `results-via-project.json` - the final detections as a\n",
        "  [VIA](https://www.robots.ox.ac.uk/~vgg/software/via/) project.  This\n",
        "  requires the images in the `frames` directory.\n",
        "\n",
        "- `results.csv` - the final detections in CSV format.\n",
        "\n",
        "- `tracks.mp4` - video with tracks (see Section 6).\n",
        "\n",
        "Note that none of those files includes the video filename.  As such,\n",
        "our recommendation is to create a results directory for each video.\n",
        "\n",
        "\n",
        "### 1.4 - GPU access\n",
        "\n",
        "A GPU is required to run this pipeline in a sensible manner.  For\n",
        "example, without a GPU, a two minutes video will take close to two\n",
        "hours to process.\n",
        "\n",
        "By default, this notebook will run with a GPU.  However, it is\n",
        "possible that you were not allocated one, typically because you've\n",
        "used up all your GPU resources.  You can confirm this, and possibly\n",
        "change it, manually.  To do that, navigate to \"Edit\" -> \"Notebook\n",
        "Settings\" and select \"GPU\" from the \"Hardware Accelerator\" menu.\n",
        "\n",
        "\n",
        "### 1.4 - Moving forward\n",
        "\n",
        "You can run this notebook on Google Colab but if you have a large\n",
        "collection of videos or if your videos are particularly long, you may\n",
        "use up all of your Colab resources.  It may be worth running this on\n",
        "your own computers.  Contact us if you need help to do that.\n",
        "Alternatively, you purchase additional \"compute units\" with [Google\n",
        "Colab Plans](https://colab.research.google.com/signup).\n"
      ],
      "metadata": {
        "id": "-tfDPTizFHUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - Setup\n"
      ],
      "metadata": {
        "id": "wWhspjzoFHUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### 2.1 - Check for GPU access\n",
        "\n",
        "#@markdown By default, this notebook will run with a GPU.  However, it\n",
        "#@markdown is possible that you were not allocated one.  If you get a\n",
        "#@markdown message saying that you do not have access to a GPU,\n",
        "#@markdown navigate to \"Edit\" -> \"Notebook Settings\" and select \"GPU\"\n",
        "#@markdown from the \"Hardware Accelerator\" menu.  Once you change it,\n",
        "#@markdown you need to run this cell again.\n",
        "\n",
        "gpu_info = !nvidia-smi --list-gpus\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "    USE_GPU = False\n",
        "    print('You are NOT connected to a GPU.  This will run very slow.')\n",
        "    print('Consider reconnecting to a runtime with GPU access.')\n",
        "else:\n",
        "    USE_GPU = True\n",
        "    print('You are connected to the following GPUs:')\n",
        "    print(gpu_info)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "k72IXhXhFHUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### 2.2 - Install and load dependencies\n",
        "\n",
        "import contextlib\n",
        "import glob\n",
        "import io\n",
        "import logging\n",
        "import os\n",
        "import os.path\n",
        "import pickle\n",
        "import shutil\n",
        "import subprocess\n",
        "import sys\n",
        "import tempfile\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "import cv2\n",
        "import google.colab.drive\n",
        "import google.colab.output\n",
        "import IPython.display\n",
        "import ipywidgets\n",
        "import matplotlib.cm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import PIL.Image\n",
        "import plotly.express\n",
        "import requests\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.cuda\n",
        "import torch.nn as nn\n",
        "import torchvision.models\n",
        "import torchvision.transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "logging.basicConfig()\n",
        "_logger = logging.getLogger()\n",
        "\n",
        "\n",
        "## The SVT package https://www.robots.ox.ac.uk/~vgg/projects/seebibyte/software/svt/\n",
        "# Despite the `--quiet` flag, it still prints out a mysterious\n",
        "# \"Preparing metadata (setup.py)\" message so just redirect stdout.\n",
        "# Important messages should go to stderr anyway.\n",
        "!pip install --quiet git+https://gitlab.com/vgg/svt/ > /dev/null\n",
        "import svt.detections\n",
        "from svt.siamrpn_tracker import siamrpn_tracker\n",
        "\n",
        "\n",
        "## The ssd.pytorch \"package\" (cloned into ssd_pytorch so it can imported)\n",
        "# When we import stuff from ssd.pytorch, it import data.coco which reads a\n",
        "# `HOME/data/coco/coco_labels.txt` (even though we don't need it).  We create\n",
        "# an empty file so it doesn't fail to import.  We also need to add `HOME` to\n",
        "# `data/config.py` or it will try to read from `/root`.  See\n",
        "# https://github.com/amdegroot/ssd.pytorch/issues/571\n",
        "!git clone --quiet \\\n",
        "  --single-branch --branch vgg-colab \\\n",
        "  https://github.com/carandraug/ssd.pytorch.git ssd_pytorch/\n",
        "!echo 'HOME = \"ssd_pytorch\"' >> ssd_pytorch/data/config.py\n",
        "!mkdir ssd_pytorch/data/coco\n",
        "!touch ssd_pytorch/data/coco/coco_labels.txt\n",
        "from ssd_pytorch.data import base_transform\n",
        "from ssd_pytorch.ssd import build_ssd\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "y3Nw3Km_FHUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### 2.3 - Mount Google Drive\n",
        "\n",
        "#@markdown When you run this cell, a dialog will appear about a\n",
        "#@markdown request for access to your Google Drive Files.  This is\n",
        "#@markdown required to access the videos for analysis and to then save\n",
        "#@markdown the results.  Once you click on \"Connect to Google Drive\",\n",
        "#@markdown a pop-up window will appear to choose a Google Account and\n",
        "#@markdown then to allow access to \"Google Drive for desktop\".\n",
        "\n",
        "google.colab.drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iOD4aWrjFHUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### 2.4 - Video File and Results Folder\n",
        "\n",
        "#@markdown To find the correct path, open the \"Files\" menu in the left\n",
        "#@markdown sidebar.  The `drive` directory contains your Google Drive\n",
        "#@markdown files.  Navigate the files, right click on the wanted file\n",
        "#@markdown or directory, and select \"Copy path\".  Then paste the path\n",
        "#@markdown in this form and run this cell.\n",
        "\n",
        "VIDEO_FILE = ''  #@param {type:\"string\"}\n",
        "RESULTS_DIRECTORY = ''  #@param {type:\"string\"}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Gk90XC-8FHUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 - Advanced Options\n",
        "\n",
        "The cells hidden in this section configure the pipeline and includes\n",
        "the advanced options.  In most cases you do not need to change them\n",
        "and can collapse them (click on the arrow drop down on the left side\n",
        "of this cell) and then click the \"Run cell\" to run all of the\n",
        "collapsed cells in one go.\n"
      ],
      "metadata": {
        "id": "7c1gbeVrFHUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #### 2.5.1 - Chimpanzee Detection\n",
        "\n",
        "#@markdown The detection step is the first step.  It detects the\n",
        "#@markdown location of chimpanzees without attempting to identify who\n",
        "#@markdown they are.\n",
        "\n",
        "#@markdown A detection model is required.  You can either train\n",
        "#@markdown your own model, or you can use one of our pre-trained\n",
        "#@markdown models for\n",
        "#@markdown [face](https://thor.robots.ox.ac.uk/models/staging/chimp-tracking/face-model-ssd300_CFbootstrap_85000.pth)\n",
        "#@markdown or\n",
        "#@markdown [body](https://thor.robots.ox.ac.uk/models/staging/chimp-tracking/body-model-ssd300_BFbootstrapBissau4p5k_prebossou_best.pth).\n",
        "#@markdown Either way, you will need to upload the model to your\n",
        "#@markdown Google Drive and specify its path here.\n",
        "DETECTION_MODEL = 'https://thor.robots.ox.ac.uk/models/staging/chimp-tracking/face-model-ssd300_CFbootstrap_85000.pth'  #@param {type: \"string\"}\n",
        "\n",
        "#@markdown When the model detects a face or body, that detection is\n",
        "#@markdown made with a confidence score.  Detections with a confidence\n",
        "#@markdown score lower than the threshold are ignored.  If you set the\n",
        "#@markdown threshold too high, you may loose some tracks but if you\n",
        "#@markdown set it too low you may gain false tracks that need to be\n",
        "#@markdown removed later.\n",
        "DETECTION_THRESHOLD = 0.37  #@param {type: \"slider\", min: 0.0, max: 1.0, step: 0.01}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SH-IeyipFHUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #### 2.5.2 - Chimpanzee Tracking\n",
        "\n",
        "#@markdown The final step is to track the detected chimpanzees in the\n",
        "#@markdown video.\n",
        "\n",
        "#@markdown You will need to provide a model.  We recommend you use\n",
        "#@markdown [this one](https://thor.robots.ox.ac.uk/models/staging/chimp-tracking/tracking-model-20181031_e45.pth).\n",
        "#@markdown You will also specify a path in your Google Drive.\n",
        "TRACKING_MODEL = 'https://thor.robots.ox.ac.uk/models/staging/chimp-tracking/tracking-model-20181031_e45.pth'  #@param {type: \"string\"}\n",
        "\n",
        "MATCH_OVERLAP_THRESHOLD = 0.6 #@param {type:\"slider\", min:0.0, max:1.0, step:0.01}\n",
        "\n",
        "UNKNOWN_TRACK_ID_MARKER = -1\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nmuWC94sFHUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #### 2.5.3 - Verbosity\n",
        "\n",
        "#@markdown How chatty do you want the notebook to be?  INFO is a good\n",
        "#@markdown choice if you want to have a feeling for progress.\n",
        "LOG_LEVEL = 'INFO'  #@param [\"CRITICAL\", \"ERROR\", \"WARNING\", \"INFO\", \"DEBUG\"]\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YUIcftRMFHUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #### 2.5.4 - The Final Setup Step\n",
        "\n",
        "#@markdown Run this cell to perform the final pipeline setup based on\n",
        "#@markdown the given options.\n",
        "\n",
        "logging.getLogger().setLevel(LOG_LEVEL)\n",
        "\n",
        "FRAMES_DIR = os.path.join(RESULTS_DIRECTORY, 'frames')\n",
        "DETECTIONS_FPATH = os.path.join(RESULTS_DIRECTORY, 'detections.pkl')\n",
        "VIA_PROJECT_FPATH = os.path.join(RESULTS_DIRECTORY, 'results-via-project.json')\n",
        "CSV_FPATH = os.path.join(RESULTS_DIRECTORY, 'results.csv')\n",
        "TRACKS_VIDEO_FPATH = os.path.join(RESULTS_DIRECTORY, 'tracks.mp4')\n",
        "\n",
        "\n",
        "# These should never be true because USE_GPU was set automatically\n",
        "# based on whether a GPU is available.\n",
        "if USE_GPU and not torch.cuda.is_available():\n",
        "    raise Exception('Your runtime does not have a GPU.')\n",
        "elif torch.cuda.is_available() and not USE_GPU:\n",
        "    _logger.warn('You have a GPU but chose to not use it.  Are you sure?')\n",
        "\n",
        "if USE_GPU:\n",
        "    DEFAULT_DEVICE = 'cuda'\n",
        "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "else:\n",
        "    DEFAULT_DEVICE = 'cpu'\n",
        "    torch.set_default_tensor_type('torch.FloatTensor')\n",
        "\n",
        "_logger.info('Will use %s device.', DEFAULT_DEVICE.upper())\n",
        "\n",
        "\n",
        "# Required to display the tracking results with plotly or matplotlib.\n",
        "google.colab.output.enable_custom_widget_manager()\n",
        "\n",
        "\n",
        "def local_path_for_model(path: str) -> str:\n",
        "    if path.startswith('https://'):\n",
        "        downloaded_fh = tempfile.NamedTemporaryFile(delete=False)\n",
        "        r = requests.get(path)\n",
        "        downloaded_fh.write(r.content)\n",
        "        downloaded_fh.flush()\n",
        "        return downloaded_fh.name\n",
        "    else:\n",
        "        return path\n",
        "\n",
        "\n",
        "DETECTION_MODEL_PATH = local_path_for_model(DETECTION_MODEL)\n",
        "TRACKING_MODEL_PATH = local_path_for_model(TRACKING_MODEL)\n",
        "\n",
        "\n",
        "face_id_normalize = torchvision.transforms.Normalize(\n",
        "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        ")\n",
        "\n",
        "\n",
        "logging2ffmpeg_loglevel = {\n",
        "    'CRITICAL': 'fatal',\n",
        "    'ERROR': 'error',\n",
        "    'WARNING': 'warning',\n",
        "    'INFO': 'info',\n",
        "    'DEBUG': 'debug',\n",
        "}\n",
        "\n",
        "FFMPEG_LOG_LEVEL = logging2ffmpeg_loglevel[LOG_LEVEL]\n",
        "\n",
        "\n",
        "def subprocess_print_stderr(args: List[str]) -> None:\n",
        "    p = subprocess.Popen(args, text=True, stderr=subprocess.PIPE)\n",
        "\n",
        "    for line in iter(p.stderr.readline, \"\"):\n",
        "        print(line, end=\"\")\n",
        "    p.wait()\n",
        "    if p.returncode != 0:\n",
        "        raise Exception('subprocess failed')\n",
        "\n",
        "\n",
        "def ffmpeg_video_to_frames(video_fpath: str, frames_dir: str) -> None:\n",
        "    subprocess_print_stderr(\n",
        "        [\n",
        "            'ffmpeg',\n",
        "            '-i', video_fpath,\n",
        "            '-vsync', 'vfr',\n",
        "            '-q:v', '1',\n",
        "            '-start_number', '0',\n",
        "            '-filter:v', 'scale=iw:ih*(1/sar)',\n",
        "            '-loglevel', FFMPEG_LOG_LEVEL,\n",
        "            # FIXME: what if %06d.jpg is not enough and rools over?\n",
        "            os.path.join(FRAMES_DIR, \"%06d.jpg\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "def ffprobe_get_frame_rate(video_fpath: str) -> float:\n",
        "    ffprobe_p = subprocess.run(\n",
        "        [\n",
        "            'ffprobe',\n",
        "            '-loglevel', 'panic',\n",
        "            '-select_streams', 'v',\n",
        "            '-show_entries', 'stream=r_frame_rate',\n",
        "            '-print_format', 'csv',\n",
        "            video_fpath,\n",
        "        ],\n",
        "        check=True,\n",
        "        stdout=subprocess.PIPE,\n",
        "        text=True,\n",
        "    )\n",
        "    # we expect something like \"stream,25/1\\n\"\n",
        "    frame_rate_parts = ffprobe_p.stdout.split(',')[1][:-1].split('/')\n",
        "    if len(frame_rate_parts) == 1:\n",
        "        return float(frame_rate_parts[0])\n",
        "    else:  # format is something such as 25/1\n",
        "        return float(frame_rate_parts[0]) / float(frame_rate_parts[1])\n",
        "\n",
        "\n",
        "def ffmpeg_video_from_frames_and_video(\n",
        "    frames_dir: str, in_video_fpath: str, out_video_fpath: str\n",
        ") -> None:\n",
        "    frame_rate = ffprobe_get_frame_rate(in_video_fpath)\n",
        "    subprocess_print_stderr(\n",
        "        [\n",
        "            'ffmpeg',\n",
        "            '-y',  # overwrite output files without asking\n",
        "            '-loglevel', FFMPEG_LOG_LEVEL,\n",
        "            '-framerate', str(frame_rate),\n",
        "            '-pattern_type', 'glob',\n",
        "            '-i', os.path.join(frames_dir, '*.jpg'),\n",
        "            '-i', in_video_fpath,\n",
        "            '-c:a', 'copy',\n",
        "            '-c:v', 'libx264',\n",
        "            '-map', '0:v:0',  # use video from input 0 / stream 0\n",
        "            '-map', '1:a:0',  # use audio from input 1 / stream 0\n",
        "            '-pix_fmt', 'yuv420p',\n",
        "            out_video_fpath,\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "# Ideally this would be a NamedTuple and values couldn't change.\n",
        "# However, in practice this is created in the detect phase and the\n",
        "# id_score is filled in during the identify phase.  So we use\n",
        "# dataclass and Optional.  Probably should be refactored.  But\n",
        "# then, SVT also wants to access the values by position so\n",
        "# dataclass brings its own problems.\n",
        "@dataclass\n",
        "class Detection:\n",
        "    track_id: int\n",
        "    x: float\n",
        "    y: float\n",
        "    w: float\n",
        "    h: float\n",
        "    id_score: Optional[float]\n",
        "\n",
        "\n",
        "# TODO: why frame_id_to_filename only includes frames with detections?\n",
        "# Can't we just check on the the list of detections that there's none?\n",
        "#\n",
        "# TODO: why do we convert the frame number to a string fo rkeys in the\n",
        "# dict?  Why not just use an int?\n",
        "#\n",
        "# TODO: why return frame_id_to_filename (can't that be deduced?)\n",
        "def detect(\n",
        "    video_frames: List[str],\n",
        "    detection_model_path: str,\n",
        "    visual_threshold: float,\n",
        "):\n",
        "    if visual_threshold < 0.0 or visual_threshold > 1.0:\n",
        "        raise ValueError(\n",
        "            'visual_threshold needs to be a number between 0.0 and 1.0'\n",
        "        )\n",
        "\n",
        "    num_classes = 2  # +1 background\n",
        "    net = build_ssd('test', 300, num_classes)\n",
        "    net.load_state_dict(\n",
        "        torch.load(detection_model_path, map_location=DEFAULT_DEVICE)\n",
        "    )\n",
        "    net.eval()\n",
        "    net = net.to(DEFAULT_DEVICE)\n",
        "\n",
        "    # XXX: Can't we do the benchmark just once and then save it?\n",
        "    if USE_GPU:\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    _logger.info('Finished loading model %s', detection_model_path)\n",
        "\n",
        "    _logger.info('Starting detection phase')\n",
        "\n",
        "    # init this important variables\n",
        "    frame_id_to_filename: Dict[str, str] = {}\n",
        "    frame_detections: Dict[str, Dict[str, Detection]] = defaultdict(dict)\n",
        "\n",
        "    # Go through frame list\n",
        "    for frame_index, frame_fpath in enumerate(video_frames):\n",
        "        frame_id = str(frame_index)  # XXX: Why?  Why are we not using the int?\n",
        "\n",
        "        if frame_index % 1000 == 0:\n",
        "            _logger.info(\n",
        "                'Starting to process images %d to %d',\n",
        "                frame_index,\n",
        "                min(frame_index + 1000 - 1, len(video_frames) - 1),\n",
        "            )\n",
        "\n",
        "        # Acquire image\n",
        "        img = cv2.imread(frame_fpath)\n",
        "\n",
        "        # Apply transforms to the image\n",
        "        transformed = base_transform(img, net.size, [104, 117, 123])\n",
        "        transformed = torch.from_numpy(transformed).permute(2, 0, 1)\n",
        "        transformed = Variable(transformed.unsqueeze(0))\n",
        "        transformed = transformed.to(DEFAULT_DEVICE)\n",
        "\n",
        "        # Silence UserWarnings from ssd_pytorch/layers/box_utils.py \"An output\n",
        "        # with one or more elements was resized since it had shape [X], which\n",
        "        # does not match the required output shape [Y]. This behavior is\n",
        "        # deprecated, and in a future PyTorch release outputs will not be\n",
        "        # resized unless they have zero elements. You can explicitly reuse an\n",
        "        # out tensor t by resizing it, inplace, to zero elements with\n",
        "        # t.resize_(0).\"\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"ignore\")\n",
        "            net_out = net(transformed)  # run detector\n",
        "        net_detections = net_out.data\n",
        "\n",
        "        # Scale each detection back up to the image\n",
        "        scale = torch.Tensor(\n",
        "            [img.shape[1], img.shape[0], img.shape[1], img.shape[0]]\n",
        "        )\n",
        "        pred_num = 0\n",
        "        for i in range(net_detections.size(1)):\n",
        "            j = 0\n",
        "            while net_detections[0, i, j, 0] >= visual_threshold:\n",
        "                score = net_detections[0, i, j, 0]\n",
        "                pt = (net_detections[0, i, j, 1:] * scale).cpu().numpy()\n",
        "                coords = (\n",
        "                    max(float(pt[0]), 0.0),\n",
        "                    max(float(pt[1]), 0.0),\n",
        "                    min(float(pt[2]), img.shape[1]),\n",
        "                    min(float(pt[3]), img.shape[0]),\n",
        "                )\n",
        "                if coords[2] - coords[0] >= 1 and coords[3] - coords[1] >= 1:\n",
        "                    if pred_num == 0:\n",
        "                        # We found at least one prediction, so add the\n",
        "                        # image to the filename dict.\n",
        "                        frame_id_to_filename[frame_id] = frame_fpath\n",
        "\n",
        "                    # XXX: original code had the option to load\n",
        "                    # face_id_score from a previous run but this makes\n",
        "                    # very little sense now?\n",
        "\n",
        "                    # Save detections to list ...\n",
        "                    a_detection = Detection(\n",
        "                        track_id=UNKNOWN_TRACK_ID_MARKER,\n",
        "                        x=coords[0],\n",
        "                        y=coords[1],\n",
        "                        w=coords[2] - coords[0],\n",
        "                        h=coords[3] - coords[1],\n",
        "                        id_score=None,\n",
        "                    )\n",
        "\n",
        "                    frame_detections[frame_id][str(pred_num)] = a_detection\n",
        "                    pred_num += 1\n",
        "                j += 1\n",
        "\n",
        "    _logger.info('Finished detections')\n",
        "    return frame_detections, frame_id_to_filename\n",
        "\n",
        "\n",
        "# TODO: not currently used because we need a notebook to fine face\n",
        "# identification first.\n",
        "def identify_faces(\n",
        "    detections: Dict[str, Dict[str, Detection]],\n",
        "    frame_id_to_filename: Dict[str, str],\n",
        "    face_id_model_fpath: str,\n",
        "):\n",
        "\n",
        "    face_id_model = torchvision.models.resnet18()\n",
        "    face_id_model.fc = nn.Linear(512, FACE_ID_CLASSES)\n",
        "    face_id_checkpoint = torch.load(face_id_model_fpath)\n",
        "\n",
        "    face_id_model = nn.DataParallel(face_id_model).cuda()\n",
        "    face_id_model.load_state_dict(face_id_checkpoint['state_dict'])\n",
        "\n",
        "    face_id_model.eval()\n",
        "    _logger.info('Finished loading model %s', face_id_model_fpath)\n",
        "    _logger.info('Starting face identification phase')\n",
        "\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    im_index_c = 0\n",
        "    for frame_id, frame_fpath in frame_id_to_filename.items():\n",
        "\n",
        "        if im_index_c % 1000 == 0:\n",
        "            _logger.info(\n",
        "                'Starting to process images %d to %d',\n",
        "                im_index_c,\n",
        "                min(im_index_c + 1000 - 1, len(frame_id_to_filename) - 1),\n",
        "            )\n",
        "        im_index_c += 1\n",
        "\n",
        "        # Acquire image (yes, again)\n",
        "        img = cv2.imread(frame_fpath)\n",
        "\n",
        "        for detection in detections[frame_id].values():\n",
        "            # Crop image to detection\n",
        "            x_end = detection.x + detection.w\n",
        "            y_end = detection.y + detection.h\n",
        "            crop_img = img[\n",
        "                int(detection.y) : int(y_end),\n",
        "                int(detection.x) : int(x_end),\n",
        "                :,\n",
        "            ]\n",
        "            # Prepare face image for classification\n",
        "            crop_img = cv2.resize(crop_img, (224, 224)).astype(np.float32)\n",
        "            zero_one_range_img = cv2.normalize(\n",
        "                crop_img,\n",
        "                None,\n",
        "                alpha=0,\n",
        "                beta=1,\n",
        "                norm_type=cv2.NORM_MINMAX,\n",
        "                dtype=cv2.CV_32F,\n",
        "            )\n",
        "            torch_permuted_img = torch.from_numpy(zero_one_range_img).permute(\n",
        "                2, 0, 1\n",
        "            )\n",
        "            crop_img_normalized = face_id_normalize(torch_permuted_img)\n",
        "            transformed = Variable(crop_img_normalized.unsqueeze(0))\n",
        "            # run face classifier\n",
        "            face_id_vector = face_id_model(transformed)\n",
        "\n",
        "            # Max's hack for bringing the results down to 0-1 range\n",
        "            face_id_vector = nn.Softmax()(face_id_vector)\n",
        "\n",
        "            # keep only relevant output + move to cpu + detach\n",
        "            face_id_vector_keep = (\n",
        "                face_id_vector[0][: len(FACE_IDENTIFICATION_MAP)]\n",
        "                .cpu()\n",
        "                .detach()\n",
        "                .numpy()\n",
        "            )\n",
        "\n",
        "            # save face_id_scores\n",
        "            detection.id_score = face_id_vector_keep\n",
        "\n",
        "    _logger.info('Finished identification')\n",
        "\n",
        "\n",
        "def track(\n",
        "    detections: Dict[str, Dict[str, Detection]],\n",
        "    frame_id_to_filename: Dict[str, str],\n",
        "    tracking_model_path: str,\n",
        "):\n",
        "    _logger.info('Starting tracking phase')\n",
        "\n",
        "    tracker_config = {\n",
        "        'gpu_id': 0 if USE_GPU else -1,\n",
        "        'verbose': True,\n",
        "        'preload_model': True,\n",
        "    }\n",
        "\n",
        "    detections_match_config = {\n",
        "        'match_overlap_threshold': MATCH_OVERLAP_THRESHOLD,\n",
        "        'UNKNOWN_TRACK_ID_MARKER': UNKNOWN_TRACK_ID_MARKER,\n",
        "        'frame_img_dir': '',\n",
        "        'verbose': True,\n",
        "        'via_project_name': VIDEO_FILE,\n",
        "    }\n",
        "\n",
        "    # We only use one shot, hence only shot_id 0.\n",
        "    # XXX: Why are we using strings for int keys?\n",
        "    shot_id = '0'\n",
        "    detections4svt = {shot_id: {}}\n",
        "    for frame_id, detections_values in detections.items():\n",
        "        detections4svt[shot_id][frame_id] = {}\n",
        "        for box_id, detection in detections_values.items():\n",
        "            detections4svt[shot_id][frame_id][box_id] = [\n",
        "                detection.track_id,\n",
        "                detection.x,\n",
        "                detection.y,\n",
        "                detection.w,\n",
        "                detection.h,\n",
        "            ]\n",
        "\n",
        "    # redirect sys.stdout to a buffer to capture the prints() in the code below\n",
        "    svt_stdout = io.StringIO()\n",
        "    with contextlib.redirect_stdout(svt_stdout):\n",
        "        tracker = siamrpn_tracker(\n",
        "            model_path=tracking_model_path, config=tracker_config\n",
        "        )\n",
        "\n",
        "        svt_detections = svt.detections.detections()\n",
        "        svt_detections.read(detections4svt, frame_id_to_filename)\n",
        "        svt_detections.match(tracker=tracker, config=detections_match_config)\n",
        "\n",
        "    _logger.info(svt_stdout.getvalue())\n",
        "    _logger.info('Finished tracking')\n",
        "    return svt_detections\n",
        "\n",
        "\n",
        "def display_detections(\n",
        "    frame_id_to_filename,\n",
        "    svt_s0_detections,\n",
        "):\n",
        "    frame_id_filename_pair = sorted(\n",
        "        list(frame_id_to_filename.items()),\n",
        "        key=lambda kv: kv[1],\n",
        "    )\n",
        "\n",
        "    figure_output = ipywidgets.Output()\n",
        "    frame_slider = ipywidgets.IntSlider(\n",
        "        value=0,\n",
        "        min=0,\n",
        "        max=len(frame_id_filename_pair) - 1,\n",
        "        step=1,\n",
        "        orientation='horizontal',\n",
        "        # description and readout are disabled because we'll show the\n",
        "        # frame filename in the label ourselves.\n",
        "        description=\"\",\n",
        "        readout=False,\n",
        "        # Only make changes when user stops moving slider.\n",
        "        continuous_update=False,\n",
        "    )\n",
        "    previous_button = ipywidgets.Button(\n",
        "        description='⮜',\n",
        "        disabled=False,\n",
        "        tooltip='Previous',\n",
        "    )\n",
        "    next_button = ipywidgets.Button(\n",
        "        description='⮞',\n",
        "        disabled=False,\n",
        "        tooltip='Next',\n",
        "    )\n",
        "\n",
        "    def show_frame(idx):\n",
        "        img = PIL.Image.open(frame_id_filename_pair[idx][1])\n",
        "        fig = plotly.express.imshow(img)\n",
        "        for track, x, y, width, height in svt_s0_detections[str(idx)].values():\n",
        "            fig.add_shape(\n",
        "                type='rect',\n",
        "                x0=x,\n",
        "                x1=x + width,\n",
        "                y0=y,\n",
        "                y1=y + height,\n",
        "                line_color='red',\n",
        "            )\n",
        "            fig.add_annotation(\n",
        "                x=x,\n",
        "                y=y,\n",
        "                text=f'Track {track}',\n",
        "                font={'color': 'red'},\n",
        "            )\n",
        "        figure_output.clear_output()\n",
        "        with figure_output:\n",
        "            fig.show()\n",
        "\n",
        "    def on_frame_slider_change(change):\n",
        "        frame_label.value = frame_id_filename_pair[change['new']][1]\n",
        "        show_frame(change['new'])\n",
        "\n",
        "    def on_click_previous(button):\n",
        "        del button\n",
        "        # IntSlider already clamps the value, we just -=1\n",
        "        frame_slider.value -= 1\n",
        "\n",
        "    def on_click_next(button):\n",
        "        del button\n",
        "        # IntSlider already clamps the value, we just +=1\n",
        "        frame_slider.value += 1\n",
        "\n",
        "    previous_button.on_click(on_click_previous)\n",
        "    next_button.on_click(on_click_next)\n",
        "    frame_slider.observe(on_frame_slider_change, names='value')\n",
        "\n",
        "    frame_label = ipywidgets.Label(frame_id_filename_pair[0][1])\n",
        "    show_frame(0)\n",
        "\n",
        "    buttons_box = ipywidgets.HBox([previous_button, frame_slider, next_button])\n",
        "    whole_box = ipywidgets.VBox([buttons_box, frame_label, figure_output])\n",
        "    IPython.display.display(whole_box)\n",
        "\n",
        "\n",
        "def make_frames_with_tracks(\n",
        "    csv_fpath: str, in_frames_dir: str, out_frames_dir: str\n",
        ") -> None:\n",
        "    cmap = list(matplotlib.cm.Pastel1.colors)\n",
        "    cmap = [(int(c[0] * 255), int(c[1] * 255), int(c[2] * 255)) for c in cmap]\n",
        "\n",
        "    font_face = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 1\n",
        "    font_thickness = 1\n",
        "\n",
        "    img = None\n",
        "    prev_frame_id = None\n",
        "\n",
        "    track_data = pd.read_csv(csv_fpath)\n",
        "    track_data.sort_values('frame_id', inplace=True)\n",
        "    for row in track_data.itertuples():\n",
        "        # Some detections have no track.  Do not show them on the\n",
        "        # video.\n",
        "        if row.track_id == UNKNOWN_TRACK_ID_MARKER:\n",
        "            continue\n",
        "\n",
        "        # Read image only if this is a different frame, otherwise,\n",
        "        # keep appending boxes and labels to the previous image.\n",
        "        if prev_frame_id != row.frame_id:\n",
        "            img = cv2.imread(os.path.join(in_frames_dir, row.frame_filename))\n",
        "            prev_frame_id = row.frame_id\n",
        "\n",
        "        colour = cmap[row.track_id % len(cmap)]\n",
        "        label = str(row.track_id)\n",
        "\n",
        "        bbox_x = int(row.x)\n",
        "        bbox_y = int(row.y)\n",
        "        bbox_w = int(row.width)\n",
        "        bbox_h = int(row.height)\n",
        "\n",
        "        text_width, text_height = cv2.getTextSize(\n",
        "            label, font_face, font_scale, font_thickness\n",
        "        )[0]\n",
        "        text_box_pts = (\n",
        "            (bbox_x + bbox_w - text_width + 4, bbox_y),\n",
        "            (bbox_x + bbox_w, bbox_y - text_height - 4),\n",
        "        )\n",
        "\n",
        "        # Draw track bounding box\n",
        "        cv2.rectangle(\n",
        "            img,\n",
        "            (bbox_x, bbox_y),\n",
        "            (bbox_x + bbox_w, bbox_y + bbox_h),\n",
        "            colour,\n",
        "            thickness=2,\n",
        "        )\n",
        "        # Draw box for bounding box label\n",
        "        cv2.rectangle(\n",
        "            img,\n",
        "            text_box_pts[0],\n",
        "            text_box_pts[1],\n",
        "            colour,\n",
        "            cv2.FILLED,\n",
        "        )\n",
        "        # Write label\n",
        "        cv2.putText(\n",
        "            img,\n",
        "            label,\n",
        "            text_box_pts[0],\n",
        "            font_face,\n",
        "            font_scale,\n",
        "            color=(0, 0, 0),\n",
        "            lineType=cv2.LINE_AA,\n",
        "        )\n",
        "        cv2.imwrite(os.path.join(out_frames_dir, row.frame_filename), img)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "OWKXpxAuFHUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 - Convert video to frames\n",
        "\n",
        "The pipeline needs the video frames as individual image files.  This\n",
        "cell will create a `frames` directory and save the individual images\n",
        "there.  You may skip running this cell if you already have a `frames`\n",
        "directory with images.  This cell will error if the `frames` directory\n",
        "already exists to prevent overwriting any existing data.\n"
      ],
      "metadata": {
        "id": "XIgX5CS8FHU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Skip this cell if you already have the frames.  If you run\n",
        "#@markdown this cell and the `frames` directory already exists, it\n",
        "#@markdown errors to avoid overwriting any previous images.\n",
        "\n",
        "os.makedirs(FRAMES_DIR, exist_ok=False)\n",
        "\n",
        "ffmpeg_video_to_frames(VIDEO_FILE, FRAMES_DIR)\n",
        "\n",
        "_logger.info('Finished extracting individual frames to \\'%s\\'', FRAMES_DIR)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "E4xj4YRWFHU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 - Detection step\n",
        "\n",
        "The detection of chimpanzees is the first step in the pipeline.  If\n",
        "you have previously run the detection step then you will have a `detections.pkl`\n",
        "file in the results directory.  If so, skip the \"detection\" cell and\n",
        "run the \"load previous detections results\" cell instead (you may need\n",
        "to click in \"2 cells hidden\" to see them).\n"
      ],
      "metadata": {
        "id": "AAhN5CkMFHU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### 4.1 - Run detection (option 1)\n",
        "\n",
        "video_frames = sorted(glob.glob(os.path.join(FRAMES_DIR, '*.jpg')))\n",
        "\n",
        "detections, frame_id_to_filename = detect(\n",
        "    video_frames,\n",
        "    DETECTION_MODEL_PATH,\n",
        "    DETECTION_THRESHOLD,\n",
        ")\n",
        "\n",
        "with open(DETECTIONS_FPATH, 'wb') as fh:\n",
        "    pickle.dump(\n",
        "        {\n",
        "            'detections': detections, \n",
        "            'frame_id_to_filename': frame_id_to_filename,\n",
        "        },\n",
        "        fh,\n",
        "    )\n",
        "_logger.info('Detection results saved to \\'%s\\'', DETECTIONS_FPATH)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jk_beBJTFHU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### 4.2 - Load previous detection results (option 2)\n",
        "\n",
        "with open(DETECTIONS_FPATH, 'rb') as fh:\n",
        "    loaded_detections = pickle.load(fh)\n",
        "\n",
        "detections = loaded_detections['detections']\n",
        "frame_id_to_filename = loaded_detections['frame_id_to_filename']\n",
        "video_frames = list(frame_id_to_filename.values())\n",
        "\n",
        "_logger.info('Detection results loaded from \\'%s\\'', DETECTIONS_FPATH)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xHVc5I4EFHU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 - Tracking step\n"
      ],
      "metadata": {
        "id": "FN5FGZxJFHU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown The final step in the pipeline is to track the detected\n",
        "#@markdown chimpanzees in the video.  At the end of this step, the\n",
        "#@markdown tracking results will be saved in a CSV file and as a\n",
        "#@markdown [VIA](https://www.robots.ox.ac.uk/~vgg/software/via/)\n",
        "#@markdown project.\n",
        "\n",
        "svt_detections = track(\n",
        "    detections,\n",
        "    frame_id_to_filename,\n",
        "    TRACKING_MODEL_PATH,\n",
        ")\n",
        "\n",
        "# XXX: So far we've been using absolute filepaths.  However, in the\n",
        "# exported VIA project we want to use the filename only so that the\n",
        "# images can be found by setting the project \"Default Path\".  This\n",
        "# hack changes the filepath in the SVT internals.\n",
        "svt_detections.frame_id_to_filename_map = {\n",
        "    k: os.path.basename(v)\n",
        "    for k, v in svt_detections.frame_id_to_filename_map.items()\n",
        "}\n",
        "\n",
        "svt_detections.export_via_project(\n",
        "    VIA_PROJECT_FPATH,\n",
        "    config={'frame_img_dir': FRAMES_DIR, 'via_project_name': ''},\n",
        ")\n",
        "svt_detections.export_plain_csv(CSV_FPATH, {})\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GodRuQQ4FHU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 - Visualise tracking results\n"
      ],
      "metadata": {
        "id": "1XoTxVlsFHU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### 6.1 - Visualise in Google Colab (option 1)\n",
        "\n",
        "#@markdown You can see the tracking results right here, inside this\n",
        "#@markdown Google Colab notebook, but the interface is a bit slow.\n",
        "#@markdown This is fine if you want to have a quick look at some of\n",
        "#@markdown of frames only.\n",
        "\n",
        "#@markdown Run this cell and then click on the arrow buttons to\n",
        "#@markdown display the next or previous frame, and you can move the\n",
        "#@markdown slider to move to a specific frame.  When you dragging the\n",
        "#@markdown slider, the displayed frame is only updated once the slider\n",
        "#@markdown is released.  Expect a couple of seconds wait for the frame\n",
        "#@markdown to be updated.\n",
        "\n",
        "display_detections(frame_id_to_filename, svt_detections.detection_data['0'])\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4sbn8DsHIi1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 - Visualise locally with VIA (option 2)\n",
        "\n",
        "[VIA](https://www.robots.ox.ac.uk/~vgg/software/via/) is a web\n",
        "application to view and perform annotations of image, audio, and\n",
        "video.  It is free software and runs locally on the web browser.  You\n",
        "can view the tracking results on the individual frames with VIA.\n",
        "\n",
        "This is much more responsive than viewing inside the notebook but\n",
        "requires download the frames locally (either manually or with [Google\n",
        "Drive for\n",
        "Desktop](https://support.google.com/a/users/answer/13022292)).\n",
        "\n",
        "1. Download [VIA\n",
        "   2](https://www.robots.ox.ac.uk/~vgg/software/via/downloads/via-2.0.12.zip).\n",
        "   This is a zip file.  Open it.  Inside there is a `via.html` file.\n",
        "   Open it in your web browser to start VIA.\n",
        "\n",
        "2. Download the `results-via-project.json` from your results diretcory\n",
        "   and the whole frames directory.  If you are using Google Drive for\n",
        "   Desktop sync it now.  The frames directory is pretty large and this\n",
        "   step may take a long time.\n",
        "\n",
        "3. Navigate to \"Project\" -> \"Load\" and select the\n",
        "   `results-via-project.json` file.  A \"File Not Found\" error message\n",
        "   will appear.  This means that VIA does not know where the images\n",
        "   are.\n",
        "\n",
        "4. Navigate to \"Project\" -> \"Settings\".  Set the \"Default Path\" to the\n",
        "   `frames` directory in your computer.\n"
      ],
      "metadata": {
        "id": "QBdj3CToLPVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### 6.3 - Create Video file with tracks (option 3)\n",
        "\n",
        "#@markdown You may also generate a video file with the detections\n",
        "#@markdown superimposed.  The video file will be named `tracks.mp4`\n",
        "#@markdown and saved in the `RESULTS_DIRECTORY` in your Google Drive.\n",
        "\n",
        "with tempfile.TemporaryDirectory() as out_frames_dir:\n",
        "    tmp_tracks_fpath = os.path.join(out_frames_dir, 'tracks.mp4')\n",
        "    make_frames_with_tracks(CSV_FPATH, FRAMES_DIR, out_frames_dir)\n",
        "    ffmpeg_video_from_frames_and_video(\n",
        "        out_frames_dir, VIDEO_FILE, tmp_tracks_fpath\n",
        "    )\n",
        "    shutil.move(tmp_tracks_fpath, TRACKS_VIDEO_FPATH)\n",
        "\n",
        "_logger.info('Video file with tracks created \\'%s\\'', TRACKS_VIDEO_FPATH)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "l44oU0LWA7nw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
